{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution Generative Adversarial Network training\n",
    "\n",
    "Here in this jupyter notebook, we will train a super-resolution generative adversarial network (SRGAN), to create a high-resolution Antarctic bed Digital Elevation Model(DEM) from a low-resolution DEM.\n",
    "In addition to that, we use additional correlated inputs that can also tell us something about the bed topography.\n",
    "\n",
    "<img src=\"https://yuml.me/diagram/scruffy;dir:LR/class/[BEDMAP2 (1000m)]->[Generator model],[REMA (200m)]->[Generator model],[MEASURES Ice Flow Velocity (450m)]->[Generator model],[Generator model]->[High res bed DEM (250m)],[High res bed DEM (250m)]->[Discriminator model],[Groundtruth Image (250m)]->[Discriminator model],[Discriminator model]->[True/False]\" alt=\"3 input SRGAN model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python       : 3.6.6 | packaged by conda-forge | (default, Oct 11 2018, 14:33:06) \n",
      "Numpy        : 1.14.5\n",
      "Keras        : 2.2.4\n",
      "Tensorflow   : 1.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import quilt\n",
    "import skimage.transform\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Add, BatchNormalization, Concatenate, Conv2D, Conv2DTranspose, Dense, Flatten, Input, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Model\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Keras        :', keras.__version__)\n",
    "print('Tensorflow   :', K.tf.__version__)\n",
    "K.tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed values\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed(seed=seed)\n",
    "K.tf.set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading package metadata...\n",
      "weiji14/deepbedmap/model/train already installed.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Overwrite? (y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragments already downloaded\n"
     ]
    }
   ],
   "source": [
    "hash = '4ec1f0a8f1b6bb0ad3f01243393aa807a7921531f5363d926b8fdd583c891f71'\n",
    "quilt.install(package='weiji14/deepbedmap/model/train', hash=hash, force=False)\n",
    "pkg = quilt.load(pkginfo='weiji14/deepbedmap', hash=hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2111, 40, 40, 1) (2111, 16, 16, 1) (2111, 8, 8, 1) (2111, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "W1_data = pkg.model.train.W1_data()  #miscellaneous data REMA\n",
    "W2_data = pkg.model.train.W2_data()  #miscellaneous data MEASURES Ice Flow\n",
    "X_data = pkg.model.train.X_data()  #low resolution BEDMAP2\n",
    "Y_data = pkg.model.train.Y_data()  #high resolution groundtruth\n",
    "#W1_data = np.load(file=\"model/train/W1_data.npy\")\n",
    "#W2_data = np.load(file=\"model/train/W2_data.npy\")\n",
    "#X_data = np.load(file=\"model/train/X_data.npy\")\n",
    "#Y_data = np.load(file=\"model/train/Y_data.npy\")\n",
    "print(W1_data.shape, W2_data.shape, X_data.shape, Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architect model\n",
    "\n",
    "Super Resolution Generative Adversarial Network model based on [Ledig et al. 2017](https://arxiv.org/abs/1609.04802v5).\n",
    "Keras implementation below takes some hints from https://github.com/eriklindernoren/Keras-GAN/blob/master/srgan/srgan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model Architecture\n",
    "\n",
    "![SRGAN architecture - Generator Network](https://arxiv-sanity-sanity-production.s3.amazonaws.com/render-output/399644/images/used/jpg/generator.jpg)\n",
    "![3-in-1 Generator Network](https://yuml.me/6ab92065.png)\n",
    "\n",
    "<!--[W2_input(MEASURES)|16x16x1]-k6n32s2 >[W2_inter|8x8x32],[W2_inter]->[Concat|8x8x96]\n",
    "[X_input(BEDMAP2)|8x8x1]-k3n32s1 >[X_inter|8x8x32],[X_inter]->[Concat|8x8x96]\n",
    "[W1_input(REMA)|40x40x1] -k15n32s5 >[W1_inter|8x8x32],[W1_inter]->[Concat|8x8x96]\n",
    "[Concat|8x8x96]->[Generator-Network],[Generator-Network]->[Y_hat(High-Resolution_DEM)|32x32x1] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(input1_shape:tuple=(8,8,1), input2_shape:tuple=(40,40,1), input3_shape:tuple=(16,16,1),\n",
    "                    num_residual_blocks:int=16, scaling:int=4, output_channels:int=1) -> keras.engine.training.Model:\n",
    "    \"\"\"\n",
    "    The generator model which is a deconvolutional neural network.\n",
    "    Converts a low resolution input into a super resolution output.\n",
    "    \n",
    "    Parameters:\n",
    "      input_shape -- shape of input tensor in tuple format (height, width, channels)\n",
    "      num_residual_blocks -- how many 'Conv2D-BatchNorm-PReLU-Conv2D-BatchNorm' blocks to use\n",
    "      scaling -- even numbered integer to increase resolution (e.g. 0, 2, 4, 6, 8)      \n",
    "      output_channels -- an integer representing number of output channels/filters/kernels\n",
    "    \n",
    "    Example:\n",
    "      An input_shape of (8,8,1) passing through 16 residual blocks with a scaling of 4\n",
    "      and output_channels 1 will result in an image of shape (32,32,1)\n",
    "    \n",
    "    >>> generator_model().input_shape\n",
    "    [(None, 8, 8, 1), (None, 40, 40, 1), (None, 16, 16, 1)]\n",
    "    >>> generator_model().output_shape\n",
    "    (None, 32, 32, 1)\n",
    "    >>> generator_model().count_params()\n",
    "    1743329\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(num_residual_blocks>=1) #ensure that we have 1 or more residual blocks\n",
    "    assert(scaling%2 == 0)  #ensure scaling factor is even, i.e. 0, 2, 4, 8, etc\n",
    "    assert(scaling>=0)  #ensure that scaling factor is zero or a positive number\n",
    "    assert(output_channels>=1)  #ensure that we have 1 or more output channels\n",
    "    \n",
    "    ## Input images\n",
    "    inp1 = Input(shape=input1_shape)  #low resolution image\n",
    "    assert(inp1.shape.ndims == 4) #needs to be shape like (?,8,8,1) for 8x8 grid\n",
    "    inp2 = Input(shape=input2_shape)  #other image (e.g. REMA)\n",
    "    assert(inp2.shape.ndims == 4) #needs to be shape like (?,40,40,1) for 40x40 grid\n",
    "    inp3 = Input(shape=input3_shape)  #other image (MEASURES Ice Flow)\n",
    "    assert(inp3.shape.ndims == 4) #needs to be shape like (?,16,16,1) for 16x16 grid\n",
    "    \n",
    "    # 0 part\n",
    "    # Resize inputs to right scale using convolution (hardcoded kernel_size and strides!)\n",
    "    inp1r = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same')(inp1)\n",
    "    inp2r = Conv2D(filters=32, kernel_size=(15,15), strides=(5,5), padding='same')(inp2)\n",
    "    inp3r = Conv2D(filters=32, kernel_size=(6,6), strides=(2,2), padding='same')(inp3)\n",
    "    \n",
    "    # Concatenate all inputs\n",
    "    #SEE https://distill.pub/2016/deconv-checkerboard/\n",
    "    X = Concatenate()([inp1r, inp2r, inp3r])  #Concatenate all the inputs together\n",
    "    \n",
    "    # 1st part\n",
    "    # Pre-residual k3n64s1 (originally k9n64s1)\n",
    "    X0 = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(X)\n",
    "    X0 = PReLU()(X0)\n",
    "    \n",
    "    # 2nd part\n",
    "    # Residual blocks k3n64s1\n",
    "    def residual_block(input_tensor):\n",
    "        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(input_tensor)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = PReLU()(x)\n",
    "        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        return Add()([x, input_tensor])\n",
    "    X = residual_block(X0)\n",
    "    for _ in range(num_residual_blocks-1):\n",
    "        X = residual_block(X)\n",
    "    \n",
    "    # 3rd part\n",
    "    # Post-residual blocks k3n64s1\n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Add()([X, X0])\n",
    "    \n",
    "    # 4th part\n",
    "    # Upsampling (if 4; run twice, if 8; run thrice, etc.) k3n256s1\n",
    "    for p, _ in enumerate(range(scaling//2), start=1):\n",
    "        X = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same')(X)\n",
    "        pixelshuffleup = lambda images: K.tf.depth_to_space(input=images, block_size=2)\n",
    "        X = Lambda(function=pixelshuffleup, name=f'pixelshuffleup_{p}')(X)\n",
    "        X = PReLU()(X)    \n",
    "    \n",
    "    # 5th part\n",
    "    # Generate high resolution output k9n1s1 (originally k9n3s1 for RGB image)\n",
    "    outp = Conv2D(filters=output_channels, kernel_size=(9,9), strides=(1,1), padding='same')(X)\n",
    "    \n",
    "    # Create model with input low-res images and output prediction\n",
    "    model = Model(inputs=[inp1, inp2, inp3], outputs=[outp], name='generator_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model Architecture\n",
    "\n",
    "Discriminator component is based on Deep Convolutional Generative Adversarial Networks by [Radford et al., 2015](https://arxiv.org/abs/1511.06434).\n",
    "Keras implementation below takes some hints from https://github.com/erilyth/DCGANs/blob/master/DCGAN-CIFAR10/dcgan.py and https://github.com/yashk2810/DCGAN-Keras/blob/master/DCGAN.ipynb\n",
    "\n",
    "![SRGAN architecture - Discriminator Network](https://arxiv-sanity-sanity-production.s3.amazonaws.com/render-output/399644/images/used/jpg/discriminator.jpg)\n",
    "\n",
    "![Discriminator Network](https://yuml.me/diagram/scruffy/class/[High-Resolution_DEM|32x32x1]->[Discriminator-Network],[Discriminator-Network]->[False/True|0/1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(input_shape:tuple=(32,32,1)) -> keras.engine.training.Model:\n",
    "    \"\"\"\n",
    "    The discriminator model which is a convolutional neural network.\n",
    "    Takes ONE high resolution input image and predicts whether it is\n",
    "    real or fake on a scale of 0 to 1, where 0 is fake and 1 is real.\n",
    "    \n",
    "    >>> discriminator_model().input_shape\n",
    "    (None, 32, 32, 1)\n",
    "    >>> discriminator_model().output_shape\n",
    "    (None, 1)\n",
    "    >>> discriminator_model().count_params()\n",
    "    6828033\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Input images\n",
    "    inp = Input(shape=input_shape)  #high resolution/groundtruth image to discriminate\n",
    "    assert(inp.shape.ndims == 4) #needs to be shape like (?,32,32,1) for 8x8 grid\n",
    "    \n",
    "    # 1st part\n",
    "    # Convolutonal Block without Batch Normalization k3n64s1\n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(inp)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    # 2nd part\n",
    "    # Convolutional Blocks with Batch Normalization k3n{64*f}s{1or2} \n",
    "    for f, s in zip([1,1,2,2,4,4,8,8], [1,2,1,2,1,2,1,2]):\n",
    "        X = Conv2D(filters=64*f, kernel_size=(3,3), strides=(s,s), padding='same')(X)\n",
    "        X = LeakyReLU(alpha=0.2)(X)\n",
    "        X = BatchNormalization()(X)\n",
    "    \n",
    "    # 3rd part\n",
    "    # Flatten, Dense (Fully Connected) Layers and Output\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units=1024)(X) #??!! Flatten?\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    outp = Dense(units=1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Create model with input highres/groundtruth images and output validity 0/1\n",
    "    model = Model(inputs=[inp], outputs=[outp], name='discriminator_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Generator and Discriminator models\n",
    "\n",
    "Here we combine the Generator and Discriminator neural network models together, and define the Perceptual Loss function where:\n",
    "\n",
    "$$Perceptual Loss = Content Loss + Adversarial Loss$$\n",
    "\n",
    "The original SRGAN paper by [Ledig et al. 2017](https://arxiv.org/abs/1609.04802v5) calculates *Content Loss* based on the ReLU activation layers of the pre-trained 19 layer VGG network.\n",
    "The implementation below is less advanced, simply using a pixel-wise [Mean Squared Error (MSE) loss](https://keras.io/losses/#mean_squared_error) as the *Content Loss*.\n",
    "Specifically, the *Content Loss* is calculated as the MSE difference between the output of the generator model (i.e. the predicted Super Resolution Image) and that of the groundtruth image (i.e. the true High Resolution Image).\n",
    "\n",
    "The *Adversarial Loss* or *Generative Loss* (confusing I know) is the same as in the original SRGAN paper.\n",
    "It is defined based on the probabilities of the discriminator believing that the reconstructed Super Resolution Image is a natural High Resolution Image.\n",
    "The implementation below uses the [Binary CrossEntropy loss](https://keras.io/losses/#binary_crossentropy).\n",
    "Specifically, this *Adversarial Loss* is calculated between the output of the discriminator model (a value between 0 and 1) and that of the groundtruth label (a boolean value of either 0 or 1).\n",
    "\n",
    "Source code for the implementations of these loss functions in Keras can be found at https://github.com/keras-team/keras/blob/master/keras/losses.py.\n",
    "\n",
    "![Perceptual Loss in a Super Resolution Generative Adversarial Network](https://yuml.me/69dc9a87.png)\n",
    "\n",
    "<!--\n",
    "[LowRes-Inputs]-Generator>[SuperResolution_DEM]\n",
    "[SuperResolution_DEM]-.->[note:Content-Loss|MeanSquaredError{bg:yellow}]\n",
    "[HighRes-Groundtruth_DEM]-.->[note:Content-Loss]\n",
    "[SuperResolution_DEM]-Discriminator>[False_or_True_Prediction]\n",
    "[HighRes-Groundtruth_DEM]-Discriminator>[False_or_True_Prediction]\n",
    "[False_or_True_Prediction]<->[False_or_True_Label]\n",
    "[False_or_True_Prediction]-.->[note:Adversarial-Loss|BinaryCrossEntropy{bg:yellow}]\n",
    "[False_or_True_Label]-.->[note:Adversarial-Loss]\n",
    "[note:Content-Loss]-.->[note:Perceptual-Loss{bg:gold}]\n",
    "[note:Adversarial-Loss]-.->[note:Perceptual-Loss{bg:gold}]\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_srgan_model(g_model:keras.engine.training.Model, d_model:keras.engine.training.Model, metrics:dict=None) -> keras.engine.training.Model:\n",
    "    \"\"\"\n",
    "    Creates a Super Resolution Generative Adversarial Network (SRGAN)\n",
    "    by joining a generator model with a discriminator model.\n",
    "    The SRGAN model will be compiled with an optimizer (e.g. Adam)\n",
    "    and have separate loss functions and metrics for its\n",
    "    generator and discriminator component.\n",
    "    \n",
    "    >>> metrics = {\"generator_model\": 'mse', \"discriminator_model\": 'accuracy'}\n",
    "    >>> srgan_model = compile_srgan_model(g_model=generator_model(), d_model=discriminator_model(), metrics=metrics)\n",
    "    >>> srgan_model.get_layer(name='generator_model').trainable\n",
    "    True\n",
    "    >>> srgan_model.get_layer(name='discriminator_model').trainable\n",
    "    False\n",
    "    >>> srgan_model.count_params()\n",
    "    8571362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that our models are named properly\n",
    "    assert(g_model.name == 'generator_model')\n",
    "    assert(d_model.name == 'discriminator_model')    \n",
    "    \n",
    "    # Compile discriminator only\n",
    "    d_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                    loss=keras.losses.binary_crossentropy)\n",
    "    d_model.trainable = False  #combined model should not train discriminator\n",
    "    \n",
    "    # Connect Generator to Discriminator\n",
    "    g_out = g_model(inputs=g_model.inputs) #g_in --(g_model)--> g_out\n",
    "    d_out = d_model(inputs=g_out)         #g_out --(d_model)--> d_out\n",
    "    \n",
    "    # Create and Compile the Super Resolution Generative Adversarial Network (SRGAN) Model!\n",
    "    model = Model(inputs=g_model.inputs, outputs=[g_out, d_out])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                  loss={\"generator_model\": keras.losses.mean_squared_error,\n",
    "                        \"discriminator_model\": keras.losses.binary_crossentropy},\n",
    "                        metrics=metrics\n",
    "                       )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Peak Signal-Noise Ratio (PSNR) metric.\n",
    "    See https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n",
    "    \n",
    "    >>> K.eval(psnr(y_true=np.ones(shape=(3,3)), y_pred=np.full(shape=(3,3), fill_value=2)))\n",
    "    array([221.80709678, 221.80709678, 221.80709678])\n",
    "    \"\"\"\n",
    "    mse = K.mean(K.square(K.np.subtract(y_pred, y_true)), axis=-1) + K.epsilon()  #add epsilon to prevent zero division\n",
    "    return K.np.multiply(20, K.log(2**16/K.sqrt(mse)))  #setting MAX_I as 2^16, i.e. max for int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8, 8, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40, 40, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 16, 16, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator_model (Model)         (None, 32, 32, 1)    1743329     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "discriminator_model (Model)     (None, 1)            6828033     generator_model[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 8,571,362\n",
      "Trainable params: 1,739,105\n",
      "Non-trainable params: 6,832,257\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session() # Reset Keras/Tensorflow graph\n",
    "metrics = {\"generator_model\": psnr, \"discriminator_model\": 'accuracy'}\n",
    "srgan_model = compile_srgan_model(g_model=generator_model(), d_model=discriminator_model(), metrics=metrics)\n",
    "srgan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='model/logs', histogram_freq=10, write_graph=True)\n",
    "model.fit(x=[X_data, W1_data, W2_data], y=Y_data, verbose=1, validation_split=0.05, batch_size=32,\n",
    "          epochs=250, callbacks=[tensorboard, PlotLossesKeras(max_cols=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(name=\"model/weights\", exist_ok=True)\n",
    "model.save(filepath=\"model/weights/srgan_full_model.hdf5\")  #model architecture and parameter weights\n",
    "model.save_weights(filepath=\"model/weights/srgan_model_weights.hdf5\")  #just the model weights\n",
    "with open(\"model/weights/srgan_model_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(x=[X_data, W1_data, W2_data], verbose=1)\n",
    "print(Y_hat.shape, Y_hat.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    try:\n",
    "        id = random.randrange(0,len(X_data))\n",
    "        print(id, X_data[id].shape)\n",
    "        \n",
    "        X_cube = skimage.transform.rescale(image=X_data[id].astype(np.int32), scale=4, order=3, mode='reflect', anti_aliasing=True, multichannel=False)\n",
    "        \n",
    "        fig, axarr = plt.subplots(nrows=1, ncols=5, squeeze=False, figsize=(15,15))\n",
    "        axarr[0, 0].imshow(X_data[id][:,:,0], aspect='equal')  #low resolution original\n",
    "        axarr[0, 1].imshow(X_cube[:,:,0], aspect='equal')      #bicubic interpolation\n",
    "        axarr[0, 2].imshow(W1_data[id][:,:,0], aspect='equal')  #REMA surface DEM\n",
    "        axarr[0, 3].imshow(Y_hat[id][:,:,0], aspect='equal')   #srcnn prediction\n",
    "        axarr[0, 4].imshow(Y_data[id][:,:,0], aspect='equal')  #groundtruth\n",
    "        \n",
    "        axarr[0, 0].set_title('BEDMAP2')\n",
    "        axarr[0, 1].set_title('Bicubic')\n",
    "        axarr[0, 1].set_xlabel(f'PSNR: {round(skimage.measure.compare_psnr(im_true=Y_data[id][:,:,0].astype(np.int32), im_test=X_cube[:,:,0].astype(np.int32)), 2)}')\n",
    "        axarr[0, 2].set_title('REMA')\n",
    "        axarr[0, 3].set_xlabel(f'PSNR: {round(skimage.measure.compare_psnr(im_true=Y_data[id][:,:,0].astype(np.int32), im_test=Y_hat[id][:,:,0].astype(np.int32)),2)}')\n",
    "        axarr[0, 3].set_title('SRCNN')\n",
    "        axarr[0, 4].set_title('Groundtruth')\n",
    "        \n",
    "        plt.show()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbedmap",
   "language": "python",
   "name": "deepbedmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
